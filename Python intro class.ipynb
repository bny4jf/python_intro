{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python vs Stata/SAS/..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we choose to use Python is forming matrices, which are usually simpler to operate with, since our estimators are often given in a matrix form.\n",
    "\n",
    "Caveat: memory usage. Your data may not fit in the memory as a matrix.\n",
    "\n",
    "SAS/Stata (also SQL) is going to execute tasks line-by-line, and defining operations that are 'vertical' can be relatively difficult. For example, in Stata you use egen, loops or the matrix language 'territory' (MATA) created for this reason. However, these elements are not optimized, or quite foreign to the original way of thinking of those softwares, which can create problems, and make code hard to read (and write).\n",
    "\n",
    "That being said, Stata is your best choice if you are doing applied economics research and do not want to venture out of the area of well-developed .ado files of econometrics papers. The econometrician wants to make your life easy, so unless Stata is really inefficient/cumbersome, they will implement the empirical strategy there.\n",
    "\n",
    "Interestingly, Stata suffers from the memory usage problem regardless, and we have more tools available (to my knowledge) in python and alike to handle it when the problem arises and we want to 'stick to matrices'. On the other hand, I believe Stata has better built-in algorithms to reduce unnecessary memory usage than other tools, so you will be able to fit 'more' data into your RAM with it. Theoretically, the line-by-line thinking and progressing lets you get rid of the memory problem, and for example SAS and SQL has that definite advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python vs R/Matlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, you would like to incorporate a machine learning element into your dissertation. In my opinion, that is a good reason to leave Stata, as canned implementations are limited in this area. However, theere is a software that is a 'language' that is dedicated to statistics: R. MatLab is good for macro people.\n",
    "\n",
    "Python is a general puprose language, which means that you have access to practical solutions coming straight from CS more directly. However, many times the implementation does not take advantage of the specificity of the problems you encounter (which are very special to us vs engineers, for example). However, this will mean that if you have a 'technical' problem (for example, need a specific data structure/file extension to be able to run you code at all) or looking for speed, generally python will be better. Partly R and definitely MatLab are famous for having some speed issues once things get tough.\n",
    "\n",
    "That being said, there is a lot of people writing stats-like code in python nowadays, and there is a huge community. However, a caveat: more people write econometrically unsound codes on their blogs, because there is not such a tight-knit community in python as in Stata (with the ado files checked by Stata) and R (everybody can be a data scientist nowadays, pheww).\n",
    "\n",
    "All in all, I would say that if you start from scratch, it does not matter which one you do. Python is easier to learn in my opinion, and it is a much more beautifully laid out language than R. If you are not very good at abstractions and do not want to learn CS jargon, R might be a better choice. If your dissertation will be the most technical thing you will ever do in your life, I do not think it matters which one you choose, and if you actually consider a career in this field, you will probably end up having to learn both tools anyway.\n",
    "\n",
    "One of the reasons why Python (or C++/...) is preferred in 'real life production' is that it allows you to organize notions/objects in a big projects efficiently. Larger IO projects nowadays use the language as it was intended: for object oriented programming (i.e. defining classes). However, much of the econometrics-statistics stuff I have seen (not to mention labor projects) will keep it simple and only define functions (on already predifined objects). There is not chance to talk about OOP in this intro, but it is one of the reasons why you would encounter python often in the 'real world'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are we here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to\n",
    "- give you some information on how to start interacting with python\n",
    "- introduce you to some of the most important packages (from our point of view)\n",
    "- give an example code of a regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools to access the processors via Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we often refer to 'Python' is really consists of the Standard Library (https://docs.python.org/3/library/) and a collection of other packages developed sometimes independently by the people who created the language.\n",
    "\n",
    "Python is a shared resource with developers from other fields, and it is very hard to keep track on what package is developed when, and what versions of packages need some specific versions of other packages. Therefore, when installing you should not go to python.org, but to one of the firms that maintain a 'distribution' of packages. These are going to take care of the dependencies between the packages you need. The distribution I use and that is also installed on your computer in the lab is by Continuum, and it is called Anaconda. An alternative is 'pip' (Unix users). It is key that you only update your packages (including the IDE!) by using conda, if you have Anaconda.\n",
    "\n",
    "1. The simplest tool to recon with is the notebook you are using now, __Jupyter__. It is great, since you can actually edit and execute JUlia, PYThon and R code with it (on different tabes). However, I mostly using Jupyter for teaching or exploring a data set, maybe. You can write 'Code' and 'Markdown' in cells. (See below.) Collaborating and version control is not that great, although if you really like it, now there is Jupyterlab which is like a more data science project version of Jupyter (haven't used it).\n",
    "\n",
    "2. The other tool that is included with Anaconda is the __Spyder__ IDE. This looks like Matlab, as it keeps trakc of all your created objects in the memory. This can be incredibly useful for us. (Note: I am pretty sure you can use Jupyter for development only if you are _really_ good, but I always use Spyder for initial development, like 80% of time, and then emacs/vim/notbeook++ later.) An alternative to Spyder would be CharmPy, which is better suited for larger projects.\n",
    "\n",
    "3. Further, you can run a python code from the command line or shell. This is a next step we are not going to cover here, but is necessary to do a mid-sized project (like your dissertation, probably).\n",
    "\n",
    "## Important packages\n",
    "\n",
    "- __ipython__: ='Interactive python'. This package makes python more convenient to use (any IDE and interactive shell will use this), enhances parallel computing capabilities and how your figures and typesetting looks like. If you can, choose an ipython kernel rather than just a python kernel. (You most probably will by default anyway.) You do not have to know anything anymore about this.\n",
    "\n",
    "- __numpy__: This is where arrays live, which are kind of like precursors for matrices. This is probably the most important package for us. It works will with Intel chips.\n",
    "- Scipy: You use this package for optimization and generating some random numbers.\n",
    "\n",
    "- __pandas__: This package defines the 'dataframe', which should be familiar from R. This is what most people with small or misized projects use for basic data set manipulation. While it can be slower than a well-optimized numpy code, it has built-in parallel computing, so in your first year or so, pandas is the way to go (unless.. you guessed well, you run out of memory - the dataframe hsa obviously more overhead).\n",
    "\n",
    "- __statsmodels__: This is where people run regressions and fit simple statistical models (some time series, very limited IV, etc).\n",
    "\n",
    "- scikit-learn: This is where you would fit simple machine learning tools. You should go through this library: https://scikit-learn.org/stable/index.html. Very nice overview, but be careful: some of the language is not how we talk in economics, and if you do weird 'preprocessing' steps, you will have a problem with your advisor. \n",
    "- nltk: The natural language processing toolkit. Not today, S, not today.\n",
    "- PIL/pillow: if you need to work with pics (pillow.)\n",
    "\n",
    "- __matplotlib__: This is how our type of people will create figures. To be specific, I have only used __matplotlib.pyplot__ in my life. That being said, I am not a plot-person.\n",
    "\n",
    "- re: Regular expressions for data cleaning. Good examples: https://www.w3schools.com/python/python_regex.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython is usually included in your environment by default. The rest of the packages are needed to be _imported_, if you would like to use them. For example by writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # You want to import all your packages at the very beginning of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic elements of the language\n",
    "\n",
    "## Simple data structures\n",
    "\n",
    "On the one hand, we have the usual stuff:\n",
    "\n",
    "str (string)\n",
    "boolean\n",
    "int\n",
    "float\n",
    "\n",
    "which are the sequences of characters and number formats, respectively. \n",
    "\n",
    "\n",
    "Already strings are acually defined as sequences, out of which we mention two main types:\n",
    "__lists__: L=[2,3,6,4, ['list', 'of' , 'a','list']]\n",
    "__tuples__: T=(3,4,55)\n",
    "\n",
    "Members of a list/tuple are delimited with commas, and numbered by starting with ZERO (the weird thing compared to R).\n",
    "This becomes important when you would like to access one or more elements of the list ('slicing'), that is done by writing brackets after the name of the list/tuple. For example L[0:2] gives the first __2__ elements of L. Of course there are already some operators defined on lists. What do you think will happen and what happened? Play around a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[3,4, 'this_is_a_string', 'Hello world!', ['another', 'list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L) # The print function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L[0:4:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2= L[4]\n",
    "print(L+L2)    # list addition (appending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after slicing, you get back a list automatically. Lists are mutable, I can redefine them, but tuples are not. Try for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup1=(2,3,4,(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup1[2]=55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could not change the 3rd element of the tuple, but the same would work with lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L[1]=55\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that lists are not the same as sets, as two lists are not the same if the order of the items is not the same, and you can of course repeat the same elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1=[1,2,3]\n",
    "L2=[3,2,1]\n",
    "if L1==L2:\n",
    "    print('They are the same')\n",
    "else:\n",
    "    print('They are not the same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If statements and loops\n",
    "\n",
    "If statements are done by 'if' and then creating a boolean, usually with a relation ($a==b$ or $a~=b$ or $a>=b$), and finishing with a colon (:). You can use of course the usual operators as well (or, and, not) and so on. If needed, the second branch can be created with an 'else:'\n",
    "\n",
    "If you would like to branch into 3 ways, you need to employ and 'elsif:' in between the 'if' and 'else' line.\n",
    "\n",
    "You signal python what is the part of code that belongs to the branch by __indenting__ (with a tab or 4 spaces); this is instead of all those curly brackets (or worse) in other languages.\n",
    "\n",
    "What do these commands do? Run them! Were you right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= int(input('Give an integer!'))\n",
    "\n",
    "if x%3 ==0 or x<=0:\n",
    "    print(x, ' is negative or divisible by 3.')\n",
    "elif x%2 ==0:\n",
    "    print(x, ' is even and not divisible by 3')\n",
    "else:\n",
    "    print( x, ' is odd and not divisible by 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops are allowing to repeat the same set of lines as many times as it is needed. There are while and for loops available (and probably more), I will only show the for loop here. You need to use the range() object for loops if you can for iteration.\n",
    "\n",
    "What does the following code do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,20,1):\n",
    "    print(i)\n",
    "    print(' Mississippi, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=list(eval(input('Give a list of numbers delimited with commas!')))\n",
    "\n",
    "sum=0\n",
    "for i in range(len(L)):\n",
    "    sum=sum+int(L[i])**2\n",
    "\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy and arrays/matrices\n",
    "\n",
    "Arrays are kind of like lists, but you can control better what type of data you put into them, they have advanced slicing capabilities (we are not going to explore here) and also, there is something called broadcasting they do. Moreover, the mathematical operations are defined now as usual elementwise operations with vectors. Arrays can be 1-dimensional, if you write arrays of arrays, then that is a 2 dimensional array, arrays of arrays of arrays are 3-dimensional, etc. A 2-dimensional array with special 'matrix' designation are going to be the matrices.\n",
    "\n",
    "Arrays are defined by the numpy module/package. They behave like n-dimensional vectors, except for multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1= np.array([1, 2,3, 4])\n",
    "arr2= np.array([34,5,1,43])\n",
    "print(arr1*arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1+arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*arr1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional arrays include 2-dimensional arrays. Indexing now requires 2 numbers, naturally. Slicing works the same way, almost (not today why is not same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3=np.array([[1,2,3],[4,5,6]])\n",
    "arr4=np.array([[5,3,4],[8,5,3]])\n",
    "print(arr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3*arr4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2[0:3]*arr3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last line is the example for broadcasting, which can be a wonderful thing. Unforunately, we cannot get into details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays are _almost_ matrices for 2-dimension. There is a separate data structure called 'matrix', it is easy to create it with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.matrix([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.matrix([[ 5,  6, 12],\n",
    "       [32, 25, 18]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x*np.transpose(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is matrix multiplication then. You can also make python think that an array is a matrix _temporarily_ by putting it into np.asmatrix(arr3) (which is just x then, for that specific usage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and importing data\n",
    "We are going to do a very simple regression. First, you need to import your data set. The data structure closest to a 'Data Set' type like in Stata or the DataFrame in R is defined by the pandas package/module. Since we are using it, of course we need to import it first.\n",
    "\n",
    "Then you want to locate the file, and tell the directory where you are working to python (in fact, you are using ipython capabilities now!!!!). Since I am sloppy, I want the data folder to be where my python code is stored. You can change directories with the usual 'cd' command (brought to you by IPython).\n",
    "\n",
    "Then you can use the import function you can see from the pandas help that matches your file type. Todd gave me this small csv data set, so we need to use.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "%cd \"C:\\Users\\ptoth\\Dropbox\\UNR 2019 Fall\\Misc\"\n",
    "\n",
    "thadata=pandas.read_csv('state_level.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see what is in the data set (first 10 rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thadata[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame is just some kind of list of records (observations), so what is the length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(thadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you refer to the 'incwage' variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thadata.incwage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it created a new DataFrame, maybe?? The point is: this is like slicing. Now let us get the 26th observation's incwage value as you should be doing it in pandas. It turns out that the type of slicing above is only used by terrible people like me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thadata.loc[26, 'incwage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thadata.loc[:,'incwage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add a constant variable to the 'X-s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thadata['constant']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details how to name variables, how to do bysort, all sorts of slicing, please see the pandas documentation. The point is, this is an environment that is very similar to the R you are used to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read in from text files, .dta and excel files pretty simply. Not so much from MatLab files. For exporting, if you would only need data for pandas usage, you should use 'pickle', which created panda's nice and really fast data file type (.pkl). You can very simply pickle and unpickle dataframes.\n",
    "\n",
    "There is a later 'version' (another package of pandas) that can accommodate somewhat larger data sets (although I am pretty sure 95% of you will not reach the limit of pandas with little effort)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honorary mention: regex for data cleaning\n",
    "Some of your data set is in Ukrainian? Some people entered phone number as (512) 3342-566 while others +3612772848? Welcome to the re module, which can solve your problem. For data cleaning problems like these, go and check it out. Fascinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running regression in Statsmodels\n",
    "To be fair, linear regression is present in sci-kit learn as well (of course). However, all the little nuances you may need for your paper are not simply implemented. Moreovre, Statsmodels is 'pandas-aware', so you can use your data frame and if you like R, it offers a similar syntax. (However, you can also use numpy arrays!)\n",
    "\n",
    "Let us import the package, and then fit an OLS model. I give a one-liner and the generic procedure with numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y= numpy.array(thadata.loc[:,'incwage'])\n",
    "#X= numpy.array(thadata.loc[:,'age']) #same as thadata['age'] here - you can be sloppy for this line.\n",
    "#X=sm.add_constant(X)\n",
    "#model= sm.OLS(endog=y, exog=X)\n",
    "#results=model.fit()\n",
    "#print(results.params)\n",
    "results= sm.OLS(thadata['incwage'], thadata[['constant','age']]).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2= sm.OLS(thadata['incwage'], thadata[['constant','age']]).fit(cov_type='HC1')\n",
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let us calculate the coefficient with pure numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y= numpy.matrix(thadata['incwage']).transpose()   # creates annoying row matrices otherwise\n",
    "X= numpy.matrix(thadata[['constant', 'age']]) # but this doesnt!\n",
    "               \n",
    "print(np.shape(y), np.shape(X))\n",
    "params=np.linalg.inv(X.transpose()* X)*X.transpose()*y\n",
    "\n",
    "print('constant parameter:', params[0])\n",
    "print('slope parameter:', params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework: Calculate the standard errors assuming homoskedasticity, then the heteroskedasticity-robust (White) standard errors!\n",
    "\n",
    "We are going to do some post-estimation work and plotting instead. First, let us calculate the residuals in the old-fashioned way, with numpy. Note that actually you could get it very simply with statsmodels.api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids= numpy.array(y-X*params) # later we need this thing as an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some plotting\n",
    "\n",
    "For plotting, you should start using just matplotlib.pyplot. So let us importb it, and do a scattor plot of the residuals and age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(thadata['age'], resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why did we do this? Which assumptions is holding/not holding from the multivariate model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see a histogram, the histogram of the residuals. Which assumption can we check this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(resids) \n",
    "plt.title(\"histogram of residuals\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulation of figures (saving it to the computer, having groups of figures) is a bit more lengthy, so we do not talk about it today. Matplotlib has excellent help. The only thing you have  to remember is that everything (including the picture above) is an object for python. You have to be able to put it into a bin: it is going to be a 'figure'. You need to call this picture a figure (belonging to the 'figure' class), and then you can save it as a png to the working folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spyder\n",
    "Ok, we have this interactive environment. But how do you write a 'do-file'? I suggest you write it in Spyder. There you have a more suitable environment to check your progress with creating new variables and results.\n",
    "\n",
    "Exercise (if time): I gave you the Wooldridge Data Set. Please read in the data from the csv file, run a regression on hprice (houseprice) with statsmodels using at first the 'basic' standard errors, then the heteroskedasticity robust standard errors. (Use every variable for the RHS.) Plot the residuals' histogram and the scatter plot between the residuals and 1 RHS variable chosen by you. What do you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further steps\n",
    "\n",
    "The packages I gave you above are very nice, because they have nice documentation, maybe the only exception is Statsmodels (but it is still accessible). You should prepare that those people call certain things in econometrics differently.\n",
    "\n",
    "I think a great resource to start learning python is this free book: Learn Python the hard way, by Zad A. Shaw. There must be a odf that is free online. However, it seems UNR subscribed to the 'premium' content of the book as well, so you only need your NetID to access the interactive thing as well. You lucky. I think you can get done with this book in a couple of weeks, and you are good to go to get acquainted with statsmodels and scikitlearn and the others. The only thing you may want to be careful with is that some of these resources are written for python 2, not python 3. You should start coding in python 3. There are not many different points at this level (print function, division and xrange vs range that comes to mind only), so it will not affect the learning as much as it seems at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for listening."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
